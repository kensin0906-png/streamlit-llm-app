import streamlit as st
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="å°‚é–€å®¶LLMã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ",
    page_icon="ğŸ‘¨â€ğŸ’¼",
    layout="wide"
)

def initialize_llm():
    """LangChain ChatOpenAI ã®åˆæœŸåŒ–"""
    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—
    api_key = os.getenv('OPENAI_API_KEY')
    
    if not api_key:
        st.error("âš ï¸ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        st.info("""
        APIã‚­ãƒ¼ã‚’è¨­å®šã™ã‚‹ã«ã¯ï¼š
        1. `.env` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        2. `OPENAI_API_KEY=your_api_key_here` ã‚’è¿½åŠ 
        3. `your_api_key_here` ã‚’å®Ÿéš›ã®APIã‚­ãƒ¼ã«ç½®ãæ›ãˆ
        """)
        return None
    
    # LangChain ChatOpenAIã®åˆæœŸåŒ–
    llm = ChatOpenAI(
        api_key=api_key,
        model="gpt-3.5-turbo",
        temperature=0.7
    )
    return llm

def get_system_message(expert_type):
    """å°‚é–€å®¶ã®ç¨®é¡ã«å¿œã˜ã¦ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ"""
    system_messages = {
        "ç¨å‹™": """
ã‚ãªãŸã¯ç¨å‹™ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ç‰¹å¾´ã‚’æŒã£ã¦å›ç­”ã—ã¦ãã ã•ã„ï¼š

- æ—¥æœ¬ã®ç¨æ³•ã«ç²¾é€šã—ãŸç¨ç†å£«ã¨ã—ã¦æŒ¯ã‚‹èˆã£ã¦ãã ã•ã„
- æ‰€å¾—ç¨ã€æ³•äººç¨ã€æ¶ˆè²»ç¨ã€ç›¸ç¶šç¨ãªã©ã®ç¨å‹™ã«é–¢ã™ã‚‹è³ªå•ã«è©³ã—ãç­”ãˆã¦ãã ã•ã„
- ç¨åˆ¶æ”¹æ­£ã‚„æœ€æ–°ã®ç¨å‹™æƒ…å ±ã«ã¤ã„ã¦ã‚‚è¨€åŠã—ã¦ãã ã•ã„
- å°‚é–€ç”¨èªã‚’ä½¿ã„ã¤ã¤ã‚‚ã€åˆ†ã‹ã‚Šã‚„ã™ã„èª¬æ˜ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„
- å¿…è¦ã«å¿œã˜ã¦å…·ä½“çš„ãªè¨ˆç®—ä¾‹ã‚„äº‹ä¾‹ã‚’ç¤ºã—ã¦ãã ã•ã„
- ç¨å‹™ç”³å‘Šã‚„ç¯€ç¨å¯¾ç­–ã«ã¤ã„ã¦ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚‚æä¾›ã—ã¦ãã ã•ã„
""",
        "çµŒç†": """
ã‚ãªãŸã¯çµŒç†ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ç‰¹å¾´ã‚’æŒã£ã¦å›ç­”ã—ã¦ãã ã•ã„ï¼š

- ç°¿è¨˜ã‚„ä¼šè¨ˆã®å°‚é–€çŸ¥è­˜ã‚’æŒã£ãŸçµŒç†ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã¨ã—ã¦æŒ¯ã‚‹èˆã£ã¦ãã ã•ã„
- ä»•è¨³ã€è²¡å‹™è«¸è¡¨ä½œæˆã€åŸä¾¡è¨ˆç®—ã€äºˆç®—ç®¡ç†ãªã©ã®çµŒç†æ¥­å‹™ã«è©³ã—ãç­”ãˆã¦ãã ã•ã„
- ä¼šè¨ˆåŸºæº–ï¼ˆæ—¥æœ¬åŸºæº–ã€IFRSç­‰ï¼‰ã«ã¤ã„ã¦ã‚‚èª¬æ˜ã—ã¦ãã ã•ã„
- çµŒç†ã‚·ã‚¹ãƒ†ãƒ ã‚„ä¼šè¨ˆã‚½ãƒ•ãƒˆã®æ´»ç”¨æ–¹æ³•ã«ã¤ã„ã¦ã‚‚ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã—ã¦ãã ã•ã„
- æœˆæ¬¡æ±ºç®—ã€å¹´æ¬¡æ±ºç®—ã®æµã‚Œã‚„æ³¨æ„ç‚¹ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„
- å†…éƒ¨çµ±åˆ¶ã‚„çµŒç†æ¥­å‹™ã®åŠ¹ç‡åŒ–ã«ã¤ã„ã¦ã‚‚è¨€åŠã—ã¦ãã ã•ã„
"""
    }
    return system_messages.get(expert_type, "ã‚ãªãŸã¯è¦ªåˆ‡ã§çŸ¥è­˜è±Šå¯Œãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚")

def generate_response(llm, expert_type, user_input):
    """LLMã‹ã‚‰ã®å¿œç­”ã‚’ç”Ÿæˆ"""
    try:
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—
        system_message_content = get_system_message(expert_type)
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ§‹ç¯‰
        messages = [
            SystemMessage(content=system_message_content),
            HumanMessage(content=user_input)
        ]
        
        # LLMã‹ã‚‰å¿œç­”ã‚’ç”Ÿæˆ
        response = llm(messages)
        return response.content
        
    except Exception as e:
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

def main():
    st.title("ğŸ‘¨â€ğŸ’¼ å°‚é–€å®¶LLMã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ")
    st.markdown("å°‚é–€åˆ†é‡ã‚’é¸æŠã—ã¦ã€ãã®é ˜åŸŸã®å°‚é–€å®¶ã¨ã—ã¦å›ç­”ã™ã‚‹LLMã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ")
    
    # LLMã®åˆæœŸåŒ–
    llm = initialize_llm()
    
    if llm is None:
        st.stop()
    
    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’2åˆ—ã«åˆ†å‰²
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.header("ğŸ”§ è¨­å®š")
        
        # å°‚é–€å®¶ã®ç¨®é¡ã‚’é¸æŠã™ã‚‹ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³
        expert_type = st.radio(
            "å°‚é–€å®¶ã®ç¨®é¡ã‚’é¸æŠã—ã¦ãã ã•ã„:",
            ["ç¨å‹™", "çµŒç†"],
            index=0,
            help="é¸æŠã—ãŸå°‚é–€åˆ†é‡ã®å°‚é–€å®¶ã¨ã—ã¦å›ç­”ã—ã¾ã™"
        )
        
        # é¸æŠã•ã‚ŒãŸå°‚é–€å®¶ã®èª¬æ˜ã‚’è¡¨ç¤º
        if expert_type == "ç¨å‹™":
            st.info("ğŸ§¾ **ç¨å‹™å°‚é–€å®¶ãƒ¢ãƒ¼ãƒ‰**\n\nç¨ç†å£«ã¨ã—ã¦ã€æ‰€å¾—ç¨ã€æ³•äººç¨ã€æ¶ˆè²»ç¨ã€ç›¸ç¶šç¨ãªã©ã®ç¨å‹™ã«é–¢ã™ã‚‹è³ªå•ã«ãŠç­”ãˆã—ã¾ã™ã€‚")
        elif expert_type == "çµŒç†":
            st.info("ğŸ“Š **çµŒç†å°‚é–€å®¶ãƒ¢ãƒ¼ãƒ‰**\n\nçµŒç†ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã¨ã—ã¦ã€ç°¿è¨˜ã€ä¼šè¨ˆã€è²¡å‹™è«¸è¡¨ã€åŸä¾¡è¨ˆç®—ãªã©ã®çµŒç†æ¥­å‹™ã«é–¢ã™ã‚‹è³ªå•ã«ãŠç­”ãˆã—ã¾ã™ã€‚")
    
    with col2:
        st.header("ğŸ’¬ è³ªå•ãƒ»ç›¸è«‡")
        
        # å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
        with st.form("query_form"):
            user_input = st.text_area(
                "è³ªå•ã‚„ç›¸è«‡å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:",
                height=150,
                placeholder=f"{expert_type}ã«é–¢ã™ã‚‹è³ªå•ã‚’ã©ã†ã..."
            )
            
            submitted = st.form_submit_button("ğŸš€ è³ªå•ã™ã‚‹", use_container_width=True)
        
        # å›ç­”ã®è¡¨ç¤ºã‚¨ãƒªã‚¢
        if submitted and user_input.strip():
            with st.spinner(f"{expert_type}å°‚é–€å®¶ã¨ã—ã¦å›ç­”ã‚’ç”Ÿæˆä¸­..."):
                # LLMã‹ã‚‰å¿œç­”ã‚’ç”Ÿæˆ
                response = generate_response(llm, expert_type, user_input)
                
                # çµæœã‚’è¡¨ç¤º
                st.success("âœ… å›ç­”ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ")
                
                # å›ç­”ã‚’è¡¨ç¤ºã™ã‚‹ã‚³ãƒ³ãƒ†ãƒŠ
                with st.container():
                    st.markdown("### ğŸ“ å°‚é–€å®¶ã‹ã‚‰ã®å›ç­”")
                    st.markdown("---")
                    st.write(response)
                    st.markdown("---")
        
        elif submitted and not user_input.strip():
            st.warning("âš ï¸ è³ªå•å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    
    # ãƒ•ãƒƒã‚¿ãƒ¼
    st.markdown("---")
    st.markdown(
        """
        <div style='text-align: center; color: gray; font-size: 0.8em;'>
        ğŸ’¡ ãƒ’ãƒ³ãƒˆ: å…·ä½“çš„ãªçŠ¶æ³ã‚„è©³ç´°ã‚’å«ã‚ã¦è³ªå•ã™ã‚‹ã¨ã€ã‚ˆã‚Šç²¾åº¦ã®é«˜ã„å›ç­”ãŒå¾—ã‚‰ã‚Œã¾ã™
        </div>
        """, 
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()